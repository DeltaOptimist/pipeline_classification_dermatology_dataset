{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Dermatology dataset in scikit-learn is a dataset used for classification tasks, specifically for diagnosing skin diseases based on various dermatological attributes. This dataset can be useful for building and evaluating machine learning models for multi-class classification problems.\n",
        "\n",
        "Here's an in-depth explanation of the Dermatology dataset:\n",
        "\n",
        "**Origin:** The Dermatology dataset is often referred to as the \"Dermatology dataset\" because it is commonly used in the field of dermatology for diagnosing skin diseases. It is available as part of the openml datasets and can be fetched using scikit-learn's `fetch_openml` function.\n",
        "\n",
        "**Data Description:**\n",
        "The Dermatology dataset contains dermatological data collected from patients, including both clinical and histopathological attributes. It is a multi-class classification dataset, meaning it is used to classify data into multiple classes or categories.\n",
        "\n",
        "Here are the key attributes of the dataset:\n",
        "\n",
        "1. **Features:** The dataset contains a total of 34 features, which are dermatological attributes used for diagnosis. These attributes include clinical and histopathological information, such as the presence or absence of various symptoms, colors, and patterns on the skin.\n",
        "\n",
        "2. **Target Variable:** The target variable is the class label, which represents the diagnosis of the skin disease. There are six possible classes (class labels) representing different skin diseases:\n",
        "   - Class 1: Psoriasis\n",
        "   - Class 2: Seboreic Dermatitis\n",
        "   - Class 3: Lichen Planus\n",
        "   - Class 4: Pityriasis Rosea\n",
        "   - Class 5: Cronic Dermatitis\n",
        "   - Class 6: Pityriasis Rubra Pilaris\n",
        "\n",
        "**Use Cases:**\n",
        "The Dermatology dataset is primarily used for training and evaluating machine learning models for multi-class classification tasks related to dermatology. Some potential use cases for this dataset include:\n",
        "\n",
        "1. **Disease Diagnosis:** It can be used to build models for automated dermatological disease diagnosis based on patient data.\n",
        "\n",
        "2. **Research:** Researchers in the field of dermatology can use this dataset to study patterns and relationships between various dermatological attributes and skin diseases.\n",
        "\n",
        "**Preprocessing and Handling Missing Values:**\n",
        "In practical applications, it's common to preprocess the dataset by handling missing values and possibly performing feature scaling or engineering. Missing values can be imputed using techniques such as mean imputation or more sophisticated methods.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "When working with the Dermatology dataset, common evaluation metrics for classification tasks can be used, including accuracy, precision, recall, F1-score, and confusion matrices.\n",
        "\n",
        "**Availability:**\n",
        "You can access the Dermatology dataset in scikit-learn using the `fetch_openml` function. It's one of the many datasets available in scikit-learn for educational and research purposes."
      ],
      "metadata": {
        "id": "BtTKxNcQD9aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n"
      ],
      "metadata": {
        "id": "567xPFzsAx0k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dermatology dataset\n",
        "data = fetch_openml(name=\"dermatology\", version=1)\n",
        "X, y = data.data, data.target.astype(int)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0sOs2xqBGho",
        "outputId": "47db032f-0a89-41e1-bfef-d513f836a5dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8gh7oIdEm_d",
        "outputId": "a2c25e5a-d1ed-484e-ac07-d1fa450090fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(366, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iwS2PcjErxO",
        "outputId": "9af80695-ab51-4986-8f32-f402a895b2b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(366,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqb9wBseExus",
        "outputId": "5714d8d0-dd57-4f74-8364-a0a4b55f1d0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "erythema                                    0\n",
              "scaling                                     0\n",
              "definite_borders                            0\n",
              "itching                                     0\n",
              "koebner_phenomenon                          0\n",
              "polygonal_papules                           0\n",
              "follicular_papules                          0\n",
              "oral_mucosal_involvement                    0\n",
              "knee_and_elbow_involvement                  0\n",
              "scalp_involvement                           0\n",
              "family_history                              0\n",
              "melanin_incontinence                        0\n",
              "eosinophils_in_the_infiltrate               0\n",
              "PNL_infiltrate                              0\n",
              "fibrosis_of_the_papillary_dermis            0\n",
              "exocytosis                                  0\n",
              "acanthosis                                  0\n",
              "hyperkeratosis                              0\n",
              "parakeratosis                               0\n",
              "clubbing_of_the_rete_ridges                 0\n",
              "elongation_of_the_rete_ridges               0\n",
              "thinning_of_the_suprapapillary_epidermis    0\n",
              "spongiform_pustule                          0\n",
              "munro_microabcess                           0\n",
              "focal_hypergranulosis                       0\n",
              "disappearance_of_the_granular_layer         0\n",
              "vacuolisation_and_damage_of_basal_layer     0\n",
              "spongiosis                                  0\n",
              "saw-tooth_appearance_of_retes               0\n",
              "follicular_horn_plug                        0\n",
              "perifollicular_parakeratosis                0\n",
              "inflammatory_monoluclear_inflitrate         0\n",
              "band-like_infiltrate                        0\n",
              "Age                                         8\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values (NaN) with the mean value for each feature\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X = imputer.fit_transform(X)"
      ],
      "metadata": {
        "id": "lRRT_EFcEfev"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[233]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E-FfrLrE5sb",
        "outputId": "77390f4e-a452-4b26-eb52-ecec0d8e048f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.,  2.,  2.,  1.,  1.,  0.,  0.,  0.,  2.,  0.,  1.,  0.,  0.,\n",
              "        2.,  0.,  1.,  2.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  0.,  1.,\n",
              "        0.,  0.,  0.,  0.,  0.,  2.,  0., 60.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Y_Seg-h8Egvr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=10000),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'MLP Neural Network': MLPClassifier(max_iter=10000),\n",
        "    'Linear Discriminant Analysis': LinearDiscriminantAnalysis()\n",
        "}\n"
      ],
      "metadata": {
        "id": "kB6PqSpHEgrt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline that includes preprocessing (scaling) and classification\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize features\n",
        "    ('classifier', None)  # The classifier will be set dynamically\n",
        "])\n"
      ],
      "metadata": {
        "id": "i7FaLhXREgmB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate each classifier using the pipeline\n",
        "for clf_name, clf in classifiers.items():\n",
        "    pipeline.set_params(classifier=clf)  # Set the classifier in the pipeline\n",
        "    pipeline.fit(X_train, y_train)  # Train the model\n",
        "    y_pred = pipeline.predict(X_test)  # Make predictions\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Evaluate accuracy\n",
        "    report = classification_report(y_test, y_pred)  # Generate classification report\n",
        "    print(f'{clf_name} Accuracy: {accuracy:.2f}')\n",
        "    print(f'Classification Report for {clf_name}:\\n{report}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-3KTfbYFLIY",
        "outputId": "02567c19-7550-48c2-b9b2-2ac40666eabe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.99\n",
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Decision Tree Accuracy: 0.99\n",
            "Classification Report for Decision Tree:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      1.00      0.98        31\n",
            "           2       1.00      1.00      1.00         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      1.00      1.00         8\n",
            "           5       1.00      0.90      0.95        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.99      0.98      0.99        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Random Forest Accuracy: 0.99\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Gradient Boosting Accuracy: 1.00\n",
            "Classification Report for Gradient Boosting:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       1.00      1.00      1.00         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      1.00      1.00         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00        74\n",
            "   macro avg       1.00      1.00      1.00        74\n",
            "weighted avg       1.00      1.00      1.00        74\n",
            "\n",
            "\n",
            "SVM Accuracy: 0.99\n",
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "K-Nearest Neighbors Accuracy: 0.99\n",
            "Classification Report for K-Nearest Neighbors:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.97      0.98        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      1.00      1.00         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.99      0.99        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Naive Bayes Accuracy: 0.91\n",
            "Classification Report for Naive Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.75      0.33      0.46         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       0.67      1.00      0.80         8\n",
            "           5       0.82      0.90      0.86        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.91        74\n",
            "   macro avg       0.87      0.87      0.85        74\n",
            "weighted avg       0.91      0.91      0.89        74\n",
            "\n",
            "\n",
            "MLP Neural Network Accuracy: 0.99\n",
            "Classification Report for MLP Neural Network:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Linear Discriminant Analysis Accuracy: 0.97\n",
            "Classification Report for Linear Discriminant Analysis:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       0.88      0.88      0.88         8\n",
            "           5       1.00      0.90      0.95        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.97        74\n",
            "   macro avg       0.96      0.96      0.96        74\n",
            "weighted avg       0.97      0.97      0.97        74\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Initialize variables to track the best model and its accuracy\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "best_classifier_name = \"\"\n",
        "\n",
        "# Train and evaluate each classifier using the pipeline\n",
        "for clf_name, clf in classifiers.items():\n",
        "    pipeline.set_params(classifier=clf)  # Set the classifier in the pipeline\n",
        "    pipeline.fit(X_train, y_train)  # Train the model\n",
        "    y_pred = pipeline.predict(X_test)  # Make predictions\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Evaluate accuracy\n",
        "    report = classification_report(y_test, y_pred)  # Generate classification report\n",
        "    print(f'{clf_name} Accuracy: {accuracy:.2f}')\n",
        "    print(f'Classification Report for {clf_name}:\\n{report}\\n')\n",
        "\n",
        "    # Save the best model based on accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = pipeline\n",
        "        best_classifier_name = clf_name\n",
        "\n",
        "# Save the best model to a pickle file\n",
        "with open(\"best_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(best_model, model_file)\n",
        "\n",
        "# Display the best algorithm and its evaluation\n",
        "print(f'Best Algorithm: {best_classifier_name}')\n",
        "print(f'Best Accuracy: {best_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyRIhct2GAWn",
        "outputId": "b65373db-24a3-4bd6-a172-a07738841034"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.99\n",
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Decision Tree Accuracy: 0.97\n",
            "Classification Report for Decision Tree:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.94      1.00      0.97        31\n",
            "           2       1.00      1.00      1.00         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      0.90      0.95        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.97        74\n",
            "   macro avg       0.99      0.96      0.97        74\n",
            "weighted avg       0.97      0.97      0.97        74\n",
            "\n",
            "\n",
            "Random Forest Accuracy: 0.99\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Gradient Boosting Accuracy: 1.00\n",
            "Classification Report for Gradient Boosting:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       1.00      1.00      1.00         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      1.00      1.00         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00        74\n",
            "   macro avg       1.00      1.00      1.00        74\n",
            "weighted avg       1.00      1.00      1.00        74\n",
            "\n",
            "\n",
            "SVM Accuracy: 0.99\n",
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "K-Nearest Neighbors Accuracy: 0.99\n",
            "Classification Report for K-Nearest Neighbors:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.97      0.98        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      1.00      1.00         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.99      0.99        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Naive Bayes Accuracy: 0.91\n",
            "Classification Report for Naive Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.75      0.33      0.46         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       0.67      1.00      0.80         8\n",
            "           5       0.82      0.90      0.86        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.91        74\n",
            "   macro avg       0.87      0.87      0.85        74\n",
            "weighted avg       0.91      0.91      0.89        74\n",
            "\n",
            "\n",
            "MLP Neural Network Accuracy: 0.99\n",
            "Classification Report for MLP Neural Network:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       1.00      0.88      0.93         8\n",
            "           5       1.00      1.00      1.00        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.99        74\n",
            "   macro avg       0.98      0.98      0.98        74\n",
            "weighted avg       0.99      0.99      0.99        74\n",
            "\n",
            "\n",
            "Linear Discriminant Analysis Accuracy: 0.97\n",
            "Classification Report for Linear Discriminant Analysis:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        31\n",
            "           2       0.90      1.00      0.95         9\n",
            "           3       1.00      1.00      1.00        13\n",
            "           4       0.88      0.88      0.88         8\n",
            "           5       1.00      0.90      0.95        10\n",
            "           6       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.97        74\n",
            "   macro avg       0.96      0.96      0.96        74\n",
            "weighted avg       0.97      0.97      0.97        74\n",
            "\n",
            "\n",
            "Best Algorithm: Gradient Boosting\n",
            "Best Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0taeElXMHvuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}